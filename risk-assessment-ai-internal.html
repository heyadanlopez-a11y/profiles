<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Adan Lopez – AI Risk Assessment</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    :root {
      --bg: #ffffff;
      --text: #1e1e1e;
      --accent: #007acc;
      --font-sans: 'Inter', 'Segoe UI', Roboto, sans-serif;
      --font-mono: 'Fira Mono', monospace;
      --card-bg: #f5f7fa;
      --border-color: #e1e4e8;
      --shadow: rgba(0,0,0,0.05);
      --radius: 6px;
    }
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600&family=Fira+Mono&display=swap');
    body {
      margin: 0;
      font-family: var(--font-sans);
      background: var(--bg);
      color: var(--text);
      line-height: 1.5;
      font-size: 16px;
      letter-spacing: -0.012em;
    }
    header {
      background: transparent;
      color: var(--text);
      padding: 2rem 1rem 1rem 1rem;
      text-align: center;
      border-bottom: 1px solid var(--border-color);
    }
    header img {
      width: 120px;
      height: 120px;
      border-radius: 50%;
      object-fit: cover;
      border: 2px solid var(--accent);
      box-shadow: 0 4px 8px var(--shadow);
      margin-bottom: 0.5rem;
    }
    header h1 {
      margin: 0.3rem 0 0 0;
      font-weight: 600;
      font-size: 1.8rem;
      color: var(--text);
    }
    header p {
      margin: 0.2rem 0 1rem 0;
      font-weight: 400;
      font-size: 1rem;
      color: #555;
      letter-spacing: 0.05em;
    }
    main {
      max-width: 720px;
      margin: 2rem auto;
      padding: 0 1rem;
    }
    section {
      background: var(--card-bg);
      padding: 1.2rem 1.5rem;
      margin-bottom: 2rem;
      border-radius: var(--radius);
      box-shadow: 0 2px 6px var(--shadow);
    }
    section h2 {
      font-weight: 600;
      font-size: 1.1rem;
      margin-bottom: 1rem;
      color: #444;
      border: none;
      padding-bottom: 0;
    }
    p, ul, ol {
      font-size: 1rem;
      color: var(--text);
      line-height: 1.6;
      margin-top: 0;
    }
    ul {
      list-style: disc;
      padding-left: 1.4rem;
      color: var(--text);
    }
    ol {
      list-style: decimal;
      padding-left: 1.8rem;
      color: var(--text);
    }
    ul li, ol li {
      margin-bottom: 0.6rem;
      padding-left: 0;
    }
    img {
      max-width: 100%;
      height: auto;
      border-radius: var(--radius);
      box-shadow: 0 4px 12px var(--shadow);
      margin-top: 10px;
    }
    a {
      color: var(--accent);
      text-decoration: none;
      font-weight: 600;
      transition: text-decoration 0.2s ease;
    }
    a:hover, a:focus {
      text-decoration: underline;
      outline: none;
    }
    footer {
      text-align: center;
      padding: 1rem 0;
      font-size: 0.85rem;
      color: #888;
      border-top: 1px solid var(--border-color);
      font-family: var(--font-mono);
    }
  </style>
</head>
<body>
<header>
  <img src="pic-profile1.jpg" alt="Adan Lopez">
  <h1>Adan Lopez</h1>
  <p>Cybersecurity | Governance | Risk | CISSP</p>
</header>
<main>
  <section>
  <h2>Understanding Risks Associated with Introduction of Private GenAI Tools in the Business</h2>
    </section>

    <section>
    <h2>Introduction</h2>
    <p>
      Organizations are eager to gain competitive advantage from AI tools, driven by promises of efficiency, automation, and new business insights from data. AI is increasingly seen as a way of reducing costs, accelerating decision-making, improving customer experience, and unlocking new revenue streams. This creates strong incentives to deploy AI quickly, often ahead of establishing mature security practices.
    </p>
    <p>
      Generative AI introduces new, unpredictable risks to businesses, particularly those on the “bleeding edge” of AI adoption. News stories are slowly increasing of sensitive data leaks, intellectual property loss, and legal exposure from offensive or harmful AI output. Meanwhile, the risks from unsanctioned use of public chatbots, “Shadow AI” are already here and will only get worse if the business fails to provide managed AI tools to discourage use of uncontrolled applications.
    </p>
    <p>
      Rapid deployment of enterprise AI comes with its own risks. Therefore, an incremental approach, starting with isolated AI tools with limited access to data is strongly encouraged, and is the primary focus of this assessment.
    </p>
  </section>

  <section>
    <h2>Scope</h2>
    <p>This assessment explores private AI models restricted to internal employees and strictly controlled data access. It considers relatively safe uses like email, document processing, and knowledgebase queries while avoiding the higher risks of public or autonomous AI.</p>
    <ul>
         <p>AI scenarios considered in this assessment:</p>
        <ul>
          <li>AI “assistant” built into Office applications to help draft documents or emails or generate formulas in Excel (i.e. Microsoft Copilot).</li>
          <li>AI-Powered Search & Summarization: Private AI chatbot connected to internal knowledgebases such as customer history, QMS and SOP document libraries, IT support knowledgebase, or HR information libraries. ChatGPT Enterprise and Microsoft Copilot Studio are popular examples. (RAG AI)</li>
          <li>Scheduling, Notes, and Transcription: Tools like MS Teams transcription offers efficiencies in notetaking, capturing action items, and searching meeting conversations.</li>
          <li>Other use cases where public access or sensitive data is not required to make use of the model.</li>
          <li>AI restricted from taking actions or committing changes (always a “human in the loop”)</li>
        </ul>
      </li>
      <p>Scenarios not considered in this assessment:</p>
        <ul>
          <li>Use of unmanaged, publicly available AI models (“Shadow AI”)</li>
          <li>Private AI models with unrestricted access to the internet, network, or data</li>
          <li>External threats from attackers using “AI enhanced” techniques against company defenses</li>
          <li>Autonomous forms of AI, such as AI “agents” or “Agentic AI”, are outside the scope of this assessment, particularly if given access to perform actions independently without human oversight.</li>
        </ul>
      </li>
    </ul>
    
    <section>
    <h2>Diagram of AI Implementation</h2>
    <p>The diagram illustrates the general architecture of the AI implementation assessed. The private AI chatbot application is firewalled from the internet and has strict access controls for who can login and run queries. The AI application is connected to various “Knowledgebases” including document folders, email, etc. The employee accesses the chatbot from the internal network and uses it for basic productivity tasks. External users outside the organization are prevented from accessing the application.</p>
    <img src="ai_architecture_diagram.png" alt="Diagram of AI Implementation">
  </section>
    
  </section>

  <section>
    <h2>Executive Summary</h2>
    <p>This section provides a high-level takeaway for business leadership. The purpose of this assessment is to inform company leaders, who may be considering AI adoption, but want a general awareness of the types of risks involved before initiating a more thorough exploration more specific to their business. This report is not intended to be an exhaustive report of all AI-related risks, but rather a general idea of risks likely to arise given a specific, limited set of use cases (see “Scope” section”). Investment decisions or detailed mitigation plans require additional due diligence. </p>
    <p><strong>Overall Risk Level: HIGH</strong></p>
    <p>High and moderate risks shown, all other threats were considered ‘Low’ (see “Risk Analysis” section below).</p>
    <img src="ai_risk_analysis_table.png" alt="Executive Risk Analysis">
  </section>

  <section>
    <h2>Context and Assumptions</h2>
    <ul>
      <li>Existing risk of shadow AI: It is assumed that users are already accessing public AI models, creating potential exposure risks for sensitive data and intellectual property. Publicly facing AI chatbots are very useful and will likely remain part of a company’s AI suite of tools. The organization should assess the risk of shadow AI separately, and implement mitigations before an incident occurs, such as training users on company policy regarding these tools, particularly regarding sensitive or controlled data, and educating them on the risks and things to look for.</li>
      <li>Unique Challenges: Securing AI presents distinct challenges because the way AI operates differs from traditional software. Unlike rule-based systems, AI prioritizes user “intent” over specific keywords and behaves non-deterministically, which makes it dangerously unpredictable. AI models frequently hallucinate, potentially misinforming or deceiving the user. Traditional boundaries between the data layer and application layer are gone, complicating visibility, monitoring, and access control. AI models cannot be fully audited or code-reviewed like standard software, making it difficult to identify vulnerabilities, prevent “prompt injection” attacks, or detect model poisoning threats. Businesses should be highly focused on training and resourcing AI initiatives appropriately to manage risks.</li>
      <li>Attacks versus Accidents: Accidents can arise from accidentally including sensitive records in training data, misconfigured models, or unpredictable behavior in novel situations; they are usually the result of human error, design flaws, or insufficient oversight. Attacks, by contrast, are deliberate exploits—like data poisoning, prompt injection, or model theft—where adversaries intentionally manipulate or misuse AI systems. While accidents often reflect gaps in governance and testing, attacks highlight the need for security controls, monitoring, and resilience against malicious actors.</li>
      <li>AI attack sophistication: Sophisticated attacks on AI are not commonly reported as of this assessment. Threat actors will become increasingly sophisticated and begin to see AI as a more valuable target, as it gains adoption. This assessment assumes that it will take some time for threat actors to develop advanced AI techniques.</li>
      <li>Lack of AI-specific expertise: Internal IT staff lack AI-specific training or experience, which is factored into this risk assessment.</li>
      <li>Costs: Cost considerations may include additional tooling to enhance security, auditing, data loss prevention, or detecting sensitive data. Additionally, IT departments may need to increase existing practices like penetration testing, monitoring, and vulnerability scanning. All of this could increase IT security budget. 
      <li>Sensitive Data, CMMC, GDPR, ITAR: Businesses should strongly consider the risks and compliance implications of processing sensitive or controlled data with AI tools. Most enterprise AI tools like MS Copilot isolate the user session and adhere to existing access permissions granted to the user by executing all transactions within the user context.  In cases where strong access control is not easily achieved, businesses should strongly consider granting the AI tools access according to the lowest level of privileges of all users who will be using the tool. 
      <li>Zero trust: Businesses should strongly consider any available methods for enforcing zero trust principles in AI tools. This could come in many forms, such as continuous user evaluation within each session, baselining AI activities to detect anomalies, a specialized AI gateway/proxy, or AI-aware UEBA detection. 
</li>
    </ul>
  </section>

  <section>
    <h2>Risk Analysis</h2>
    <p>The following risks are identified and the likelihood, impact, and risk levels of each is scored according to the "Assessment Scale" section below. Each risk is described along with the basic rationale for the scores given. </p>
    <img src="ai_risk_analysis_table1.png" alt="Risk Analysis">
    <img src="ai_risk_analysis_table2.png" alt="Risk Analysis">
  </section>
  
  <section>
    <h2>Assessment Scale</h2>
    <p>Simplified version of NIST Special Publication 800-30 “Guide for Conducting Risk Assessments.”</p>
    <p>The risks are scored according to this assessment scale roughly based on NIST 800-30, which is intended for US defense. I simplified the scale a bit to suit the business. The "Sources" section links to some authoritative sources which provided the bases for mapping threats to the appropriate likelihood and impact. Scoring the risk of any given threat is very specific to the individual business, it's operating environment, and what it's dependencies are related to it's key business drivers. Definitely work closely with business managers to score the threats appropriately for your business.”</p>
    <img src="ai_assessment_scale.png" alt="Assessment Scale">
  </section>

  <section>
    <h2>Risk Mitigation Considerations</h2>
    <ol>
      <li>Slow Incremental Adoption: Consideration should be given to beginning with personal internal AI tools that do not access sensitive or regulated information, gradually expanding capability once risks are managed and controls validated. Ultimately, effective enterprise adoption requires integrating AI risk management and data protection controls, monitoring use, and raising awareness of these emerging threats among all staff.</li>
      <li>Enterprise buy-in and messaging: It’s important that leadership at each business unit is aware of the risks and are receiving direction from corporate regarding safe adoption and providing appropriate support. Business units can benefit from sharing experiences, ideas, and knowledge to increase adoption across the enterprise while each subsequent business benefits from the ones leading the charge.</li>
      <li>Shared Responsibility</li>
      <li>Consider waiting on AI in email applications: Email poses an elevated risk due to its direct exposure to emails from outside the organization. An attacker could craft an email containing a “prompt injection” attack which would be directly read by the AI. Until email filtering solutions can block these kinds of attacks, the organization should consider the risks carefully before exposing the AI directly to email coming from outside the organization. Email is an extremely common and compelling use case, so if waiting is not an option, proceed with appropriate safeguards and consider getting outside expertise.</li>
      <li>Update IR/DRP Plan: Incident response procedures generally require only slight changes in training and procedures to address AI risks. However, AI increases exposure to supply chain risk, which typically can only be mitigated with strong incident response and disaster recovery planning. Test those plans and make sure it will work when you need it.</li>
      <li>Guard Rails and Input/Output Filtering: Put technical and policy boundaries in place that limit what AI systems can do, reducing the chance of harmful or unintended outputs. Examples include restricting access to certain functions, limiting external connections, and using policy enforcement layers that block unsafe actions. There is an emerging ecosystem of tools in this area that IT teams can investigate. Screen what goes into and comes out of the AI system to block malicious prompts, sensitive data, or unsafe content. This often requires layered filtering, such as regex checks, data loss prevention (DLP) tools, and bias/harm detection tuned specifically for AI workflows.</li>
      <li>AI User Training and Risk Awareness: Train employees on safe use of AI, making them aware of risks like data leakage, bias, or overreliance on outputs. Encourage a “trust but verify” approach so staff validate AI responses before acting on them. Critically, make sure to update the company employee policies and Acceptable Use Policies to clarify how employees are allowed to interact with private and public AI technology.</li>
      <li>Change Control of Training Data: Careful</li>
      <li>AI-specific IT Training: This is perhaps the most critical. Equip IT and security teams with specialized skills to configure, monitor, and secure AI systems properly. Governance of AI will be crucial to ensuring safe practices, but it’s a completely new skill set. Training should include adversarial testing, monitoring for model drift, and awareness of emerging AI threat vectors.</li>
      <li>Zero Trust and Existing Security Frameworks: Integrate AI tools into existing security practices and wherever possible, enforce the principles of Zero Trust. Limit who can interact with or manage the AI model to reduce the risk of misuse or insider threats. Access should follow least-privilege principles and may require multi-factor authentication and logging for model interactions. Enable continuous validation of user sessions, verify as much as possible any emails or data fed into the model or used for model training. Utilize risk management practices, continuous improvement, and all the normal compliance frameworks to protect data.</li>
      <li>Restricting AI Access to Data: Enforce strict controls on which data the AI can access to prevent exposure of sensitive or regulated information. This means using data classification, fine-grained access policies, and careful separation of training vs. production data sets. The risk of AI implementation is directly proportionate to the data it can access.</li>
      <li>Limited Autonomy: Set clear boundaries on the level of autonomy given to AI systems, ensuring critical decisions remain under human oversight. Implement human-in-the-loop review for sensitive actions and restrict AI decision-making in high-stakes areas like finance, HR, or legal.</li>
      <li>Model Monitoring & Drift Detection: Continuously monitor AI outputs for anomalies, bias, or performance degradation. Implement drift detection tools to flag when models behave differently than expected due to changes in data or environment.</li>
      <li>Red Teaming & Adversarial Testing: Conduct regular red-team exercises focused on AI, including prompt injection, model extraction, and data poisoning attempts. This helps expose vulnerabilities unique to AI systems before attackers do. Conduct AI-aware penetration tests to uncover vulnerabilities in how the model, data, and integrations are protected. Testing should include prompt injection attempts, model inversion, and supply-chain risks, not just traditional network exploits.</li>
      <li>Data Provenance & Lineage Tracking: Track the origin, quality, and handling of data used for AI training and inference. Strong data governance reduces the risk of poisoning, leakage, or regulatory violations.</li>
      <li>Third-Party & Supply Chain Assurance: Vet external AI services, APIs, and pre-trained models for security and compliance. Require vendors to provide documentation on testing, bias controls, and patching practices. Adversarial red teaming and bias/fairness auditing by companies like <a href="https://www.qualitestgroup.com/solutions/content-moderation/">Qualitest Group</a> or <a href="https://www.cigniti.com/services/ai-based-application-testing/">Cigniti Technologies</a> mitigate AI risks by using external experts to simulate attacks, probe for harmful behaviors, and independently evaluate models for bias, fairness, and regulatory compliance.</li>
      <li>Logging & Audit Trails for AI Interactions: Maintain detailed logs of prompts, outputs, and administrative actions on AI systems. These records support investigations, audits, and regulatory reporting when incidents occur.</li>
    </ol>
  </section>


  <section>
    <h2>AI Threat Model Diagram</h2>
    <p>Diagram from The OWASP AI Exchange: “The below diagram puts the controls in the AI Exchange into groups and places these groups in the right lifecycle with the corresponding threats.”<br>
    <a href="https://owaspai.org/docs/ai_security_overview/#ai-security-matrix">0. AI Security Overview – AI Exchange</a>
    </p>
    <img src="ai_threat_model.png" alt="AI Threat Model Diagram">
  </section>

  <section>
    <h2>Sources</h2>
    <p><strong>MITRE Atlas</strong><br>
      MITRE ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems) is MITRE’s knowledge base that documents real-world tactics, techniques, and case studies of how adversaries attack or misuse AI and machine learning systems. It works much like MITRE ATT&CK but focused on AI.<br>
      <a href="https://atlas.mitre.org/matrices/ATLAS">https://atlas.mitre.org/matrices/ATLAS</a>
    </p>
    <p><strong>NIST AI Risk Management Framework (AI RMF 1.0)</strong><br>
      The AI RMF provides a structured approach to managing risks across the lifecycle of AI systems, with an emphasis on trustworthiness, accountability, and transparency. It outlines core functions—Govern, Map, Measure, and Manage—to help organizations identify risks, evaluate impacts, and implement controls. The framework is flexible and designed for use across sectors, serving as a foundational guide for assessing and mitigating AI-related risks.<br>
      <a href="https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf?utm_source=chatgpt.com">Artificial Intelligence Risk Management Framework (AI RMF 1.0)</a>
    </p>
    <p><strong>NIST Generative AI Profile (AI 600-1)</strong><br>
      This profile builds on the AI RMF and tailors its principles specifically to generative AI systems. It highlights risks unique to these models, such as hallucinations, bias amplification, intellectual property misuse, and malicious prompt injection. The profile offers practical considerations and mappings to help organizations apply the RMF in real-world scenarios where generative AI tools are being developed or deployed.<br>
      <a href="https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf?utm_source=chatgpt.com">Artificial Intelligence Risk Management</a>
    </p>
    <p><strong>OWASP Top 10 for Large Language Model Applications</strong><br>
      The OWASP LLM Top 10 identifies the most pressing security risks for applications built on large language models, including issues like prompt injection, insecure output handling, training data poisoning, model denial of service, supply chain vulnerabilities, sensitive data disclosure, insecure plugin design, excessive agency, overreliance, and model theft.<br>
      <a href="https://owasp.org/www-project-ai-security-and-privacy-guide/">OWASP AI Security and Privacy Guide | OWASP Foundation</a>
    </p>
  </section>

  <section>
    <h2>CMMC, ITAR, GDPR</h2>
    <h3>CMMC</h3>
    <p>The following controls should be carefully considered if planning to process CUI data with. These controls are not allowed to be on a Plan of Actions and Milestones (POA&M), and pose a risk of losing CMMC certification and potentially DoD contracts. The business should consider engaging with a C3PAO to discuss potential compliance risks. The business should consider avoiding CUI data altogether unless absolutely necessary.</p>
    <ul>
      <li>AC.L2-3.1.5 – Least Privilege</li>
      <li>AU.L2-3.3.1 – System Auditing</li>
      <li>AU.L2-3.3.2 – User Accountability</li>
      <li>CM.L2-3.4.6 – Least Functionality</li>
      <li>RA.L2-3.11.2 – Vulnerability Scan</li>
      <li>SI.L2-3.14.6 – Monitor Communications for Attacks</li>
      <li>SI.L2-3.14.7 – Identify Unauthorized Use</li>
    </ul>

    <h3>GDPR</h3>
    <p>The following GDPR requirements are identified as being potentially a problem for PII stored, processed, or accessed within an AI model. Models do not “purge” data easily, nor are they particularly amenable to auditing, accountability, or accuracy requirements. The business should avoid using AI to process PII data unless the risks are well understood.</p>
    <ul>
      <li><strong>Accuracy (Article 5(1)(d))</strong>: The chatbot could generate inaccurate or misleading outputs about individuals, which may result in processing errors that conflict with GDPR’s accuracy requirement.</li>
      <li><strong>Storage Limitation & Retention (Article 5(1)(e))</strong>: If the chatbot logs or stores conversations, PII could be retained longer than necessary, violating storage limitation rules. Data retention policies may not be easily enforced in AI applications.</li>
      <li><strong>Integrity & Confidentiality (Article 5(1)(f), Article 32)</strong>: Internal chatbots must ensure robust technical and organizational safeguards. Risks include unauthorized access to chat logs, model inversion attacks, or improper access rights.</li>
      <li><strong>Right to Rectification and Erasure (Articles 16–17)</strong>: If PII is embedded in model training data or logs, it can be extremely difficult to correct or delete it, creating compliance challenges for data subject rights.</li>
      <li><strong>Accountability & Documentation (Article 5(2), Article 24)</strong>: GDPR requires proof that compliance measures are in place. Without clear logging, policies, and access controls, it’s hard to demonstrate accountability.</li>
    </ul>

    <h3>ITAR</h3>
    <p>A GenAI chatbot handling ITAR-controlled data must be designed with strict safeguards to avoid unlawful “exports” and unauthorized access. Key risks include deemed exports if foreign persons can access the system, cloud hosting outside the U.S., uncontrolled retention of ITAR data in logs or training sets, weak access controls, insufficient auditability, and exposure through third-party services. To minimize these risks, organizations should implement the chatbot within a U.S.-only, on-premises or ITAR-compliant cloud environment, enforce U.S. person–only access with strong authentication, disable or tightly control logging and model retraining to prevent data persistence, and maintain comprehensive audit trails of all interactions. These controls may not be easily implemented in an AI application. The business should consider avoiding ITAR data entirely in AI models.</p>
  </section>
</main>
<footer>
  © 2025 Adan Lopez – Cybersecurity Profile
</footer>
</body>
</html>
